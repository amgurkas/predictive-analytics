---
title: "Homework Batch 2"
author: "Alyssa Gurkas, Deirdre Flynn, Marc Fridson"
output: 
  word_document:
    reference_docx: cerulean-style.docx
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,         # Show R code in the document
  results = 'markup',  # Show R output
  warning = FALSE,     # Suppress warning messages
  error = FALSE,       # Suppress error messages
  message = FALSE,     # Suppress messages
  fig.align = "center" # Center-align all figures
)

# Set theme
ggplot2::theme_set(ggplot2::theme_minimal() + 
  ggplot2::theme(plot.title = ggplot2::element_text(size = 14, face = "bold"),
        axis.title = ggplot2::element_text(size = 12)))

# insert libraries used here
library(tidyverse)
library(AppliedPredictiveModeling)
library(caret)
library(glmnet)
library(mlbench)
library(lattice) 
library(nnet)
library(corrplot)
library(rpart)
library(VIM)
```

# KJ 6.3

*Instructions*: A chemical manufacturing process for a pharmaceutical product was discussed in Sect. 1.4. In this problem, the objective is to understand the relationship between biological measurements of the raw materials (predictors), measurements of the manufacturing process (predictors), and the response of product yield. Biological predictors cannot be changed but can be used to assess the quality of the raw material before processing. On the other hand, manufacturing process predictors can be changed in the manufacturing process. Improving product yield by 1% will boost revenue by approximately one hundred thousand dollars per batch:

## KJ 6.3(a)

*Instructions*: Start R and use these commands to load the data: \> library(AppliedPredictiveModeling) \> data(chemicalManufacturingProcess)

The matrix processPredictors contains the 57 predictors (12 describing the input biological material and 45 describing the process predictors) for the 176 manufacturing runs. Yield contains the percent yield for each run.

### Approach - KJ 6.3(a)

The appropriate libraries will be loaded per the instructions. We will evaluate the dimensions of the data (number of rows and observations), confirm that yield is in the 1st column and is the output, and create a new data frame that excludes it.

### Analysis - KJ 6.3(a)

```{r kj-6.3.a.-analysis}
data(ChemicalManufacturingProcess)

dim(ChemicalManufacturingProcess)

yield <- ChemicalManufacturingProcess$Yield
predictors <- ChemicalManufacturingProcess[, names(ChemicalManufacturingProcess) != "Yield"]
```

## KJ 6.3(b)

*Instructions*: A small percentage of cells in the predictor set contain missing values. Use an imputation function to fill in these missing values (e.g., see Sect. 3.8).

### Approach - KJ 6.3(b)

We will use the knnImpute function to fill in the missing values and create a data frame to hold the data for the next step. The k-Nearest Neighbor (KNN) approach predicts a new sample using the K-closest (typically in distance) samples from the training set. By default, this function uses the 10 nearest neighbors which is what we will use in our analysis.

### Analysis - KJ 6.3(b)

```{r kj-6.3.b.-analysis}
preProc <- preProcess(predictors, method = "knnImpute")
predictors_imputed <- predict(preProc, newdata = predictors)
```

## KJ 6.3(c)

*Instructions*: Split the data into a training and a test set, pre-process the data, and tune a model of your choice from this chapter. What is the optimal value of the performance metric?

### Approach - KJ 6.3(c)

We will use the caret package to split the data into a test set and a training set, using 80% of the data for training (both the predictors (x) and the response (y), and will ensure that the distribution of the response data in yield is similar in both sets.

We will use the ridge regression function on this data to predict the response. Ridge regression is useful when you have many highly correlated predictors. It works by adding a penalty to the model so coefficients don't get too large which prevent it from overfitting. There is a tuning parameter in ridge regression called lambda. If lambda is large, the coefficients are shrunk towards zero, although all predictors are kept in the model. We will tune the parameters to find the configuration that leads to the smallest RMSE, which is our performance metric.

Cross validation is a common technique using multiple different combinations of data (folds) to test the model. In 10 fold cross validation we train the model on 9 folds and test on the 10th. The goal is to find the lambda that gives us the smallest RMSE.

### Analysis - KJ 6.3(c)

```{r kj-6.3.c.-analysis}

set.seed(123)
trainIndex <- createDataPartition(yield, p = 0.8, list = FALSE)
X_train <- predictors_imputed[trainIndex, ]
X_test  <- predictors_imputed[-trainIndex, ]
y_train <- yield[trainIndex]
y_test  <- yield[-trainIndex]

ridge_grid <- expand.grid(alpha = 0, lambda = 10^seq(-3, 3, length = 100))

set.seed(123)
ridge_model <- train(
  x = X_train, y = y_train,
  method = "glmnet",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = ridge_grid,
  metric = "RMSE"
)
ridge_model$bestTune
min(ridge_model$results$RMSE)
```

The optimal tuning parameter for lambda is 2.15 which produced the lowest cross validated RMSE of 1.36 which indicates the model performed well using the ridge regression approach.

## KJ 6.3(d)

*Instructions*: Predict the response for the test set. What is the value of the performance metric and how does this compare with the resampled performance metric on the training set?

### Approach - KJ 6.3(d)

Next we will test this ridge model on the remaining data (X_test, and y_test from above), the test set. We are looking for the lowest RMSE and a high R squared.

### Analysis - KJ 6.3(d)

```{r kj-6.3.d.-analysis}
y_pred <- predict(ridge_model, newdata = X_test)

rmse_test <- RMSE(y_pred, y_test)
r2_test <- R2(y_pred, y_test)
rmse_cv <- min(ridge_model$results$RMSE)

cat("Test RMSE:", round(rmse_test, 3), "\n")
cat("Test R²:", round(r2_test, 3), "\n")
cat("Train RMSE (CV):", round(rmse_cv, 3), "\n")
```

The test RMSE is 1.299 which is lower than it was on the training set at 1.36. This indicates that the model generalizes well and that it may have underestimated its performance on the test data. The r squared (.50) means that the model explains 50% of the variation in the response.

## KJ 6.3(e)

*Instructions*: Which predictors are most important in the model you have trained? Do either the biological or process predictors dominate the list?

### Approach - KJ 6.3(e)

In ridge regression all variables stay in the model but some coefficients are shrunk towards zero, indicating they lower importance. We will run the model and look for the coefficients with the highest values, either positive or negative. In the output below I choose to list the 5 most influential variables.

### Analysis - KJ 6.3(e)

```{r kj-6.3.e.-analysis}
ridge_coef <- coef(ridge_model$finalModel, s = ridge_model$bestTune$lambda)
coef_df <- as.data.frame(as.matrix(ridge_coef))
coef_df$Feature <- rownames(coef_df)
colnames(coef_df)[1] <- "Coefficient"

coef_df <- coef_df[coef_df$Feature != "(Intercept)", ]
top_predictors <- coef_df[order(abs(coef_df$Coefficient), decreasing = TRUE), ]
head(top_predictors, 5)
```

The most influential predictors and their coefficients are:

ManufacturingProcess32: 0.222

ManufacturingProcess17: -0.192

ManufacturingProcess09: 0.187

ManufacturingProcess06: 0.179

ManufacturingProcess13:-0.174

Manufacturing predictors dominate the list.

## KJ 6.3(f)

*Instructions*: Explore the relationships between each of the top predictors and the response. How could this information be helpful in improving yield in future runs of the manufacturing process?

### Approach - KJ 6.3(f)

We will look at the value of the coefficients paying particular attention to those that are largest or smallest.

### Analysis - KJ 6.3(f)

The coefficients that are negative indicate that the process has a negative impact on product yield, meaning that reducing this variable could improve output. The largest positive coefficients mean that increasing those variables (or improving those manufacturing processes) could also increase yield. Refinements in these manufacturing processes, especially those with negative signs, may have the most impact on output.

# KJ 7.2

*Instructions*: Friedman (1991) introduced several benchmark data sets create by simulation. One of these simulations used the following nonlinear equation to create data: y =10 sin(πx1x2) + 20(x3 − 0.5)2 +10x4 +5x5 +N(0,σ2) where the x values are random variables uniformly distributed between [0, 1] (there are also 5 other non-informative variables also created in the simulation).

Tune several models on these data. Which models appear to give the best performance? Does MARS select the informative predictors (those named X1–X5)?

### Approach - KJ 7.2

We will tune three different models on these data using the KNN approach, the MARS approach, and the neural network approach and will compare RMSE and r-squared to determine the best approach. To begin, the predictor matrix will be converted to a data frame for better readability and visualization, and we will use pair plots and a correlation heatmap to explore relationships among predictors.

### Analysis - KJ 7.2

```{r kj-7.2-analysis}
set.seed(123)
trainingData <-mlbench.friedman1(200, sd = 1) 
 ## We convert the 'x' data from a matrix to a data frame > ## One reason is that this will give the columns names. 

trainingData$x <-data.frame(trainingData$x) ## Look at the data using featurePlot
  
featurePlot(x = trainingData$x, y = trainingData$y, plot = "pairs")

## This creates a list with a vector 'y' and a matrix > ## of predictors 'x'. Also simulate a large test set to > ## estimate the true error rate with good precision: > 

set.seed(123)
testData <-mlbench.friedman1(5000, sd = 1)
testData$x <-data.frame(testData$x)

featurePlot(
  x = trainingData$x[, 1:5],
  y = trainingData$y,
  plot = "pairs"
)

corrplot(cor(trainingData$x), method = "color")

```

The feature plot function recommended by the book created a scatter plot that is essentially useless, especially with the uninformative predictors. A subsequent plot removes them, but the usefulness remains unchanged. A correlation plot is at least legible, but again not that useful for data analysis.

We begin by tuning the KNN Model. This model predicts a value by averaging the outcomes of the k closest training points to a new observation. Pre-processing is used to center and scale the data. We chose a tune length of 10 meaning that 10 different values of the nearest neighbors will be tried.

```{r kj-7.2-analysis knn}
set.seed(123)
knnModel <-train(
x = trainingData$x,
y = trainingData$y, 
method = "knn",
preProcess = c("center", "scale"),
tuneLength = 10)
knnModel
```

The model used RMSE to select the optimal number of neighbors using the smallest value. The final model uses the 15 closest (by distance) training points (the nearest neighbors) to predict the outcome after centering and scaling the data as it had the smallest RMSE of 3.10. An issue with this approach is that it will evaluate the (by design) non-informative variables included in the data set.

```{r kj-7.2-analysis knn2}
knnPred <-predict(knnModel, newdata = testData$x) 
## The function 'postResample' can be used to get the test set ## performance values 
postResample(pred = knnPred, obs = testData$y)
```

On the test data, the KNN approach achieved an RMSE of 3.2. The R-squared of 0.65 means the model explains roughly 65% of the variation in the test set outcomes.

Next we tune the model using the MARS approach which builds piecewise linear regression models that can include hinge points to better fit the data. No pre-processing of data is needed in this approach.

```{r kj-7.2-analysis MARS}
set.seed(123)
marsModel <- train(
  x = trainingData$x,
  y = trainingData$y,
  method = "earth",
  tuneGrid = expand.grid(nprune = 2:25, degree = 1:2),
  trControl = trainControl(method = "boot", number = 25)
)
marsModel
```

Similarly, RMSE was used to select the optimal model using the smallest value. The model selected 12 "basis functions" or "pieces" (controlled by parameter "nprune") which represents the number of terms retained in the model and fit a quadratic model to each piece (hence the degree = 2).

```{r kj-7.2-analysis MARS2}
marsPred <- predict(marsModel, newdata = testData$x)
postResample(pred = marsPred, obs = testData$y)
summary(marsModel$finalModel)
```

The MARS model only selects informative variables, and it did correctly select only the informative variables (X1-X5) as set up in the problem. It also identified multiple hinge points, which lets the model change slope at that point.

On the test data, the MARS approach achieved an RMSE of 1.2. The R-squared of 0.94 means the model explains roughly 94% of the variation in the test set outcomes. With a small RMSE of 1.2 and a large R-squared, this model does a very good job of explaining variation in the data and appears to fit the data better than the KNN approach does.

Next we evaluate the neural net model and will need to center and scale the data as we did with KNN. This approach uses "weight decay" to penalize large coefficients to dampen the risk of overfitting.

```{r kj-7.2-analysis Neural}
set.seed(123)
nnetModel <- train(
  x = trainingData$x,
  y = trainingData$y,
  method = "nnet",
  preProcess = c("center", "scale"),
  linout = TRUE,     # continuous output, not classification
  trace = FALSE,     # suppress training output
  tuneLength = 10,
  trControl = trainControl(method = "boot", number = 25)
)
nnetModel
```

RMSE was used to select the optimal model by choosing the smallest value. The final model used 1 hidden unit and a decay parameter of 0.1. The hidden unit is part of the model’s hidden layer that helps it learn complex relationships in the data. The decay is a weight penalty that helps avoid overfitting by shrinking weights.

```{r kj-7.2-analysis Neural2}
nnetPred <- predict(nnetModel, newdata = testData$x)
postResample(pred = nnetPred, obs = testData$y)
```

On the test data, the neural network achieved an RMSE of 2.67. The R-squared of 0.71 means the model explains roughly 71% of the variation in the test set outcomes. RMSE is higher than it was in the MARS model and the R-squared is lower.

The MARS model appears to give the best performance and only uses informative predictors.

# KJ 7.5

*Instructions*: Exercise 6.3 describes data for a chemical manufacturing process. Use the same data imputation, data splitting, and pre-processing steps as before and train several nonlinear regression models.

## KJ 7.5(a)

*Instructions*: Which nonlinear regression model gives the optimal resampling and test set performance?

### Approach

In Exercise 6.3,the chemical manufacturing dataset missing predictor values were 
handled through imputing the values using k-nearest neighbors. Then, the data 
was split into training and test sets, and ridge regression was applied with 
10-fold cross-validation to model the relationship between predictors and product 
yield. Model performance was evaluated using RMSE and R², and identified the most influential process predictors affecting yield.

To determine which nonlinear regression model gives the optimal resampling and 
test set performance, these steps can be followed:
- Train multiple nonlinear regression models such as Random Forest or Neural Net.
- Use resampling techniques during training like k-fold cross-validation to estimate each model’s performance on unseen data.
- Collect the resampling performance metrics (i.e., lowest RMSE or highest R²).
- Apply each trained model to the test set.
- Compare both resampling and test set performance.
- Check for consistency between resampling and test results.

This can be performed by developing a function that:
(1) Trains the model using the train() function.
(2) Defines tuning parameters
(3) Extracts performance metrics (RMSE, R^2)
(4) Returns a list with model and performance metrics.

### Analysis - KJ 7.5(a)

```{r kj-7.2.a.-analysis}
### Using 6.3 analysis from above:
# Load the dataset
data(ChemicalManufacturingProcess)

# Separate response and predictors
yield <- ChemicalManufacturingProcess$Yield
predictors <- ChemicalManufacturingProcess[, names(ChemicalManufacturingProcess) != "Yield"]
# Impute missing values using k-NN
set.seed(123)
preProc <- preProcess(predictors, method = "knnImpute")
predictors_imputed <- predict(preProc, newdata = predictors)
# Split into Train/Test Sets
set.seed(123)
trainIndex <- createDataPartition(yield, p = 0.8, list = FALSE)

X_train <- predictors_imputed[trainIndex, ] # train set predictors
X_test  <- predictors_imputed[-trainIndex, ] # test set predictors
y_train <- yield[trainIndex] # train set response
y_test  <- yield[-trainIndex] # test set response

# creating a function that trains the model, extracts resampling performance metrics, and returns the model object and metrics.
train_and_evaluate <- function(X_train, y_train, method_name, X_test, y_test) {
  training_data <- data.frame(X_train, y = y_train)
  
  tr_control <- trainControl(
    method = "cv",
    number = 10,
    verboseIter = FALSE
  )
  
  tune_grid <- switch(
    method_name,
    "rf" = expand.grid(mtry = c(2, 4, 6)),
    "svmRadial" = expand.grid(sigma = c(0.01, 0.05), C = c(0.25, 0.5, 1)),
    "nnet" = expand.grid(size = c(5, 7, 9, 12), decay = c(0, 0.01, 0.1, 0.5)),
    "knn" = expand.grid(k = c(3, 5, 7, 9)),
    NULL
  )
  
  model_fit <- train(
    y ~ .,
    data = training_data,
    method = method_name,
    trControl = tr_control,
    tuneGrid = tune_grid,
    metric = "RMSE"
  )
  
  # Predict on test set
  predictions <- predict(model_fit, newdata = X_test)
  
  # Calculate test metrics
  test_metrics <- data.frame(
    RMSE = RMSE(predictions, y_test),
    Rsquared = R2(predictions, y_test)
  )
  
  list(
    model = model_fit,
    best_rmse = min(model_fit$results$RMSE),
    test_metrics = test_metrics
  )
}

results <- lapply(models, function(m) {
  train_and_evaluate(X_train, y_train, m, X_test, y_test)
})

names(results) <- models

test_perf <- lapply(models, function(m) {
  results[[m]]$test_metrics |> 
    mutate(model = m)
}) |> 
  bind_rows()
```

## KJ 7.5(b)

*Instructions*: Which predictors are most important in the optimal nonlinear regression model? Do either the biological or process variables dominate the list? How do the top ten important predictors compare to the top ten predictors from the optimal linear model?

### Approach - KJ 7.5(b)

[insert approach explanation here].

### Analysis - KJ 7.5(b)

```{r kj-7.2.b.-analysis}
# insert analysis here
```

## KJ 7.5(c)

*Instructions*: Explore the relationships between the top predictors and the response for the predictors that are unique to the optimal nonlinear regression model. Do these plots reveal intuition about the biological or process predictors and their relationship with yield?

### Approach - KJ 7.5(c)

[insert approach explaination here].

### Analysis - KJ 7.5(c)

```{r kj-7.2.c.-analysis}
# insert analysis here
```

# KJ 8.1 Recreate the simulated data from Exercise 7.2.

## KJ 8.1(a)

*Instructions*: Fit a random forest model to all of the predictors, then estimate the variable importance scores. Did the random forest model significantly use the uninformative predictors (V6 – V10)?

### Approach - KJ 8.1(a)

[insert approach explaination here].

### Analysis - KJ 8.1(a)

```{r kj-8.1.a.-analysis}
# insert analysis here
```

## KJ 8.1(b)

*Instructions*: Now add an additional predictor that is highly correlated with one of the informative predictors. Fit another random forest model to these data. Did the importance score for V1 change? What happens when you add another predictor that is also highly correlated with V1?

### Approach - KJ 8.1(b)

[insert approach explaination here].

### Analysis - KJ 8.1(b)

```{r kj-8.1.b.-analysis}
# insert analysis here
```

## KJ 8.1(c)

*Instructions*: Use the cforest function in the party package to fit a random forest model using conditional inference trees. The party package function varimp can calculate predictor importance. The conditional argument of that function toggles between the traditional importance measure and the modified version described in Strobl et al. (2007). Do these importances show the same pattern as the traditional random forest model?

### Approach - KJ 8.1(c)

[insert approach explaination here].

### Analysis - KJ 8.1(c)

```{r kj-8.1.c.-analysis}
# insert analysis here
```

## KJ 8.1(d)

*Instructions*: Repeat this process with different tree models, such as boosted trees and Cubist. Does the same pattern occur?

### Approach - KJ 8.1(d)

[insert approach explaination here].

### Analysis - KJ 8.1(d)

```{r kj-8.1.d.-analysis}
# insert analysis here
```

# KJ 8.2

*Instructions*: Use a simulation to show tree bias with different granularities.

### Approach - KJ 8.2

This question is asking us to run an experiment, or simulation, that shows how decision trees are biased, using different approaches to breaking up the predictor space, or granularities. A shallow tree with very few splits has "coarse" granularity which means that it is at risk of underfitting the data and missing patterns or interactions systematically (bias). However, "fine" granularity, or a deep tree, can over fit the data, making it less useful as a tool for prediction as it doesn't generalize well. We will use the data from the Friedman data set and will fit both a small tree (low granularity) and a big tree (high granularity), and will check RSME to evaluate performance. We will visualize the results to emphasize the difference.

### Analysis - KJ 8.2

```{r kj-8.2.-analysis}
# this replicates what was done in question 7.2 to create usable data in a data frame
set.seed(123)
trainingData <- mlbench.friedman1(200, sd = 1) 
trainingData <- data.frame(trainingData$x, y = trainingData$y)

set.seed(123)
testData <- mlbench.friedman1(5000, sd = 1)
testData <- data.frame(testData$x, y = testData$y)

# We will fit a shallow tree (coarse granularity, high bias) and determine RMSE
shallow_tree <- rpart(y ~ ., data = trainingData,
                      control = rpart.control(maxdepth = 2))
shallow_preds <- predict(shallow_tree, newdata = testData)
rmse_shallow <- RMSE(shallow_preds, testData$y)

# We will fit a deep tree (fine granularity, lower bias) and determine RMSE.
deep_tree <- rpart(y ~ ., data = trainingData,
                   control = rpart.control(cp = 0.0001))
deep_preds <- predict(deep_tree, newdata = testData)
rmse_deep <- RMSE(deep_preds, testData$y)

# We will print and compare RMSE
cat("Shallow Tree RMSE:", round(rmse_shallow, 3), "\n")
cat("Deep Tree RMSE:", round(rmse_deep, 3), "\n")

# Let's visually compare the actual data to the predictions by the shallow tree to see signs of bias
plot_data <- data.frame(
  Actual = testData$y,
  Pred_Shallow = shallow_preds,
  Pred_Deep = deep_preds
)

ggplot(plot_data, aes(x = Actual, y = Pred_Shallow)) +
  geom_point(alpha = 0.5, color = "red") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(title = "Shallow Tree Predictions Show Bias",
       x = "Actual y", y = "Predicted y") +
  theme_minimal()

ggplot(plot_data, aes(x = Actual, y = Pred_Deep)) +
  geom_point(alpha = 0.4, color = "blue") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  labs(title = "Deep Tree: Predicted vs Actual",
       x = "Actual y", y = "Predicted y") +
  theme_minimal()

```
The shallow tree produced an RMSE of 4.2 and the deep tree produced an RMSE of 3.6. This shows that with a deeper tree, you decrease RMSE, reducing bias. 

In the graphics, the dashed line represents the ideal case in which the predicted values all equal the actual values. In our shallow tree, we only allowed a maximum depth of two which results in four possible terminal nodes and only four possible predictions, leading to four horizontal lines/bands which are far from the ideal line, indicating bias. The deep tree mode, with more options to split the data, produces a wider range of predictions, meaning it fits the data more closely, indicating reduced bias. 


# KJ 8.3

*Instructions*: In stochastic gradient boosting the bagging fraction and learning rate will govern the construction of the trees as they are guided by the gradient. Although the optimal values of these parameters should be obtained through the tuning process, it is helpful to understand how the magnitudes of these parameters affect magnitudes of variable importance. Figure 8.24 provides the variable importance plots for boosting using two extreme values for the bagging fraction (0.1 and 0.9) and the learning rate (0.1 and 0.9) for the solubility data. The left-hand plot has both parameters set to 0.1, and the right-hand plot has both set to 0.9:

## KJ 8.3(a)

*Instructions*: Why does the model on the right focus its importance on just the first few of predictors, whereas the model on the left spreads importance across more predictors?

### Approach - KJ 8.3(a)

[insert approach explaination here].

### Analysis - KJ 8.3(a)

```{r kj-8.3.a.-analysis}
# insert analysis here
```

## KJ 8.3(b)

*Instructions*: Which model do you think would be more predictive of other samples?

### Approach - KJ 8.3(b)

[insert approach explaination here].

### Analysis - KJ 8.3(b)

```{r kj-8.3.b.-analysis}
# insert analysis here
```

## KJ 8.3(c)

*Instructions*: How would increasing interaction depth affect the slope of predictor importance for either model in Fig. 8.24?

### Approach - KJ 8.3(c)

[insert approach explaination here].

### Analysis - KJ 8.3(c)

```{r kj-8.3.c.-analysis}
# insert analysis here
```

# KJ 8.4

*Instructions*: Use a single predictor in the solubility data, such as the molecular weight or the number of carbon atoms and fit several models. Plot the predictor data versus the solubility results for the test set. Overlay the model predictions for the test set. How do the model differ? Does changing the tuning parameter(s) significantly affect the model fit?

## KJ 8.4(a)

### Approach - KJ 8.4(a)

[insert approach explaination here].

### Analysis - KJ 8.4(a)

```{r kj-8.4.a.-analysis}
# insert analysis here
```

## KJ 8.4(b)

### Approach - KJ 8.4(b)

[insert approach explaination here].

### Analysis - KJ 8.4(b)

```{r kj-8.4.b.-analysis}
# insert analysis here
```

## KJ 8.4(c)

### Approach - KJ 8.4(c)

[insert approach explaination here].

### Analysis - KJ 8.4(c)

```{r kj-8.4.c.-analysis}
# insert analysis here
```

# KJ 8.7

*Instructions*: Refer to Exercises 6.3 and 7.5 which describe a chemical manufacturing process. Use the same data imputation, data splitting, and pre-processing steps as before and train several tree-based models:

## KJ 8.7(a)

### Approach - KJ 8.7(a)

[insert approach explaination here].

### Analysis - KJ 8.7(a)

```{r kj-8.7.a.-analysis}
# insert analysis here
```

## KJ 8.7(b)

*Instructions*: Which predictors are most important in the optimal tree-based regression model? Do either the biological or process variables dominate the list? How do the top 10 important predictors compare to the top 10 predictors from the optimal linear and nonlinear models?

### Approach - KJ 8.7(b)

[insert approach explaination here].

### Analysis - KJ 8.7(b)

```{r kj-8.7.b.-analysis}
# insert analysis here
```

## KJ 8.7(c)

*Instructions*: Plot the optimal single tree with the distribution of yield in the terminal nodes. Does this view of the data provide additional knowledge about the biological or process predictors and their relationship with yield?

### Approach - KJ 8.7(c)

[insert approach explaination here].

### Analysis - KJ 8.7(c)

```{r kj-8.7.c.-analysis}
# insert analysis here
```
