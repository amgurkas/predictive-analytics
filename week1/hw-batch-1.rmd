
---
title: "Homework Batch 1"
author: "Alyssa Gurkas, Deirdre Flynn, Marc Fridson"
output: 
  word_document:
    reference_docx: cerulean-style.docx
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,         # Show R code in the document
  results = 'markup',  # Show R output
  warning = FALSE,     # Suppress warning messages
  error = FALSE,       # Suppress error messages
  message = FALSE,     # Suppress messages
  fig.align = "center" # Center-align all figures
)

# Set theme
ggplot2::theme_set(ggplot2::theme_minimal() + 
  ggplot2::theme(plot.title = ggplot2::element_text(size = 14, face = "bold"),
        axis.title = ggplot2::element_text(size = 12)))

library(fpp2)
library(tidyverse)
library(forecast)
library(caret)
library(mlbench)
```

# HA 2.1 Use the help function to explore what the series `gold`, `woolyrnq` and `gas` represent.

```{r 2.1-help-gold}
help("gold")
```

The gold dataset represents the daily gold prices in US dollars from January 1, 
1985 to March 31, 1989.

```{r 2.1-help-woolyrnq}
help("woolyrnq")
```

The woolyrnq dataset represents the quarterly Australian production of woollen 
yarn (in tonnes) from March 1965 - September 1994. 

```{r 2.1-help-gas}
help("gas")
```

The gas dataset represents the Australian monthly gas production from 1956-1995.

## HA 2.1.a. Use `autoplot()` to plot each of these in separate plots.

### HA 2.1.a. Approach
For this problem, each datasets will be autoplotted separately. 

### HA 2.1.a. Analysis
##### `gold` Plot: Daily Gold Prices in US dollars: January 1, 1985 to March 31, 1989
```{r 2.1a.gold-plot}
autoplot(gold)+
  xlab("Time") + ylab("U.S. Dollars")
```

##### `woolyrnq` Plot: Quarterly Australian Production of Woollen Yarn (in tonnes) from March 1965 - September 1994
```{r 2.1a.woolyrnq-plot}
autoplot(woolyrnq)+
  xlab("Time") + ylab("Woollen Yarn Production (Tonnes)")
```

##### `gas` Plot: Australian Monthly Gas Production from 1956-1995
```{r 2.1a.gas-plot}
autoplot(woolyrnq)+
  xlab("Time") + ylab("Gas Production")
```

## HA 2.1.b. What is the frequency of each series? Hint: apply the `frequency()` function.

### HA 2.1.b. Approach
To view the frequency of each series, you can use the frequency function from
the stats package. This is helpful in determining what type of data you are working with for example, daily, monthly, or annual data. 

### HA 2.1.b. Analysis
```{r 2.1.b.freq}
frequency(gold)
frequency(woolyrnq)
frequency(gas)
```

Gold Frequency: `r stats::frequency(gold)`
Woollen Yarn Frequency: `r stats::frequency(woolyrnq)`
Gas Frequency: `r stats::frequency(gas)`

#### HA 2.1.c. Use which.max() to spot the outlier in the gold series. Which observation was it?
```{r 2.1.c.outlier-gold}
which.max(gold)
```
The outlier in the gold series is `r which.max(gold)` U.S. Dollars.

# HA 2.3 Download some monthly Australian retail data from the book website. These represent retail sales in various categories for different Australian states, and are stored in a MS-Excel file.

## HA 2.3.a. You can read the data into R with the following script:
```{r 2.3a}
retaildata <- readxl::read_excel("data/retail.xlsx", skip=1)
# The second argument (skip=1) is required because the Excel sheet has two header rows.
```

## HA 2.3.b. Select one of the time series as follows (but replace the column name with your own chosen column):

```{r select-series}
myts <- ts(retaildata[,"A3349335T"],
  frequency=12, start=c(1982,4))
```

## HA 2.3.c. Explore your chosen retail time series these functions: autoplot(), ggseasonplot(), ggsubseriesplot(), gglagplot(), ggAcf(). Can you spot any seasonality, cyclicity and trend? What do you learn about the series?

#### Time Series Plot: Retail Price
```{r retail-plot}
autoplot(myts)+ 
  xlab("Time") + ylab("Retail Price")
```

```{r myts-outlier}
which.max(myts)
```
This means that the 381st row is the maximum value in this dataset. 

#### Time Series Seasonality Plot: Retail Price
```{r retail-season-plot}
ggseasonplot(myts,
             year.labels=TRUE, year.labels.left=TRUE)+ 
  xlab("Time") + ylab("Retail Price")
```
A seasonal plot is useful for understanding the underlying seasonal pattern. 
In this plot we can see that there is an increase in the product's price in 
December, January, and a slight increase in March.

#### Time Series Subseries Plot: Retail Price
```{r retail-subseriesplot}
ggsubseriesplot(myts)+ 
  xlab("Time") + ylab("Retail Price")
```
A seasonal subseries plot can also be used to understand seasonal patterns. In
Figure 6, the data for each season is collected together in time plots. As 
depicted in Figure 6, there is a spike in the product price in December, January,
and a slight increase in March.

#### Lag Plot: Retail Price
```{r retail-lagplot}
gglagplot(myts,
          lags=6)+ 
  xlab("Time") + ylab("Retail Price")
```
A lag plot helps assess autocorrelation by plotting a time series against its lagged values. The strong linear patterns across lags in the plot above suggests the series is highly predictable based on past values.

#### AFC Plot: Retail Price
```{r retail-Acf}
ggAcf(myts)
```
The ACF plot shows significant autocorrelation at all lags, indicating strong persistence in the data and justifying the use of past values in forecasting models.

# KJ 3.1
*Problem Introduction*
The UC Irvine Machine Learning Repository6 contains a data set related
to glass identification. The data consist of 214 glass samples labeled as one
of seven class categories. There are nine predictors, including the refractive
index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.
The data can be accessed via:
```{r KJ.3.1}
data(Glass)
str(Glass)
```

## KJ 3.1.a Using visualizations, explore the predictor variables to understand their distributions as well as the relationships between predictors

### KJ 3.1.a Approach 
- Plot the data to see the distribution using a histogram. This can be done by using the ggplot facet_wrap() argument, or by creating individual plots.
- Alternatively, the data can be visualized using the ggpairs() function from the GGally package. This provides a matrix of containing density plots, scatterplots, Pearson correlations, and boxplots among indicators.

### KJ 3.1.a Analysis 
```{r KJ.3.1.a-analysis}
glass2 <- Glass %>% 
  select(-Type) %>% 
  pivot_longer(everything(), names_to = "indicator", values_to = "value")

# facet wrapped indicators
ggplot(glass2, aes(value)) +
  geom_histogram(bins = 70, fill = "steelblue", color = "black") +
  facet_wrap(~indicator)

# density plots - doesn't show anything b/c values are not spread similarly
ggplot(glass2) +
  geom_density(aes(x=value,fill=indicator)) +
  labs(
    x="Value",
    y="Density"
  )
  ggtitle("Density Plots of Indicators") +
  theme_bw() +
  theme(axis.text.x = element_text(face = 'bold', size = 10),
        axis.text.y = element_text(face = 'bold', size = 10))

# Individual Indicators
plot_glass_distributions <- function(data) {
  numeric_data <- data[sapply(data, is.numeric)]
  
  for (colname in names(numeric_data)) {
    p <- ggplot(data, aes_string(x = colname)) +
      geom_histogram(bins = 30, fill = "steelblue", color = "black") +
      theme_minimal() +
      labs(title = paste("Distribution of", colname),
           x = colname,
           y = "Frequency")
    print(p)  # Print each plot individually
    readline(prompt = "Press [Enter] to show the next plot...")
  }
}

# running function, need to save each ggplot so that it can go into the word doc
# plots should be included in the Appendix.
plot_glass_distributions(Glass)

# plotting the indicators using the ggpairs() function
Glass %>% 
  select(-Type) %>% 
GGally::ggpairs()
```
From these plots, it is apparent that not all of the predictors are normally distributed, and that some have outliers, such as Ba, Al, and Na. Additionally, from the scatterplot matrix produced by the ggpairs() function, it is apparent that certain predictors have stronger relationships than others. 

## KJ 3.1.b Do there appear to be any outliers in the data? Are any predictors skewed?

### KJ 3.1.b Approach 
Outliers can be detected through visualizations like boxplots. By plotting the outliers in red, it is apparent how many there are, and if the data may need to be transformed. 

### KJ 3.1.b Analysis
```{r KJ.3.1.b-analysis}
ggplot(glass2, aes(x = "", y = value)) +
  geom_boxplot(fill = "lightblue", outlier.color = "red") +
  facet_wrap(~ indicator, scales = "free_y") +
  theme_minimal() +
  labs(title = "Boxplots of Predictors",
       y = "Value",
       x = "") +
  theme(strip.text = element_text(size = 8))
```
# From this plot, it is apparent that there are outliers in nearly all of the 
# predictors except for Mg. To handle skewnewss, the data can be transformed
# through Log or Box-Cox transformations. 

## KJ 3.1.c Are there any relevant transformations of one or more predictors that might improve the classification model?

### KJ 3.1.c Approach
There are a few transformations or steps that can be taken to improve the classification model: 
- *Data Cleaning*: Check missing values (Removal versus imputation)
- *Data Transformation*: Center the data or scale it. For features that are skewed, can perform Log or Box-Cox transformations to normalize distributions.
- *Identify Problematic Predictors*: Filter data for near-zero variance predictors (these are predictors where most of the values are the same), and highly correlated predictors that may lead to collinearity issues and increase model variance. 

### KJ 3.1.c Analysis
#### Data Cleaning: Check for duplicate and missing values 
```{r KJ 3.1.c.-missing-values}
# Check for missing values
na_counts <- map_dfc(Glass, ~ sum(is.na(.x)))
names(na_counts) <- paste0("NA_", names(Glass))

distinct_counts <- map_dfc(Glass, ~ n_distinct(.x, na.rm = TRUE))
names(distinct_counts) <- paste0("T_", names(Glass))

sum_missing <- bind_cols(na_counts, distinct_counts)
```
From the `sum_missing` dataframe, we can see that no indicators have missing values, which means imputations are not necessary. 

#### Data Transformation: Centering and Scaling Data
Note: cannot use the glass2 version because the PreProcess() function is intended for wide datasets. When typing trans into the console, we can see that nine indicators were center and scaled, and that one indicator was ignored. 
```{r KJ 3.1.c.-analysis-transformations}
trans <- preProcess(Glass, method=c("center","scale"))

# Can add in the Box-Cox method as well by adding it into the method arugment
trans <- preProcess(Glass, method=c("center", "scale", "BoxCox"))
```
To identify problematic predictors we can filter data for near-zero variance predictors (these are predictors where most of the values are the same), and highly correlated predictors that may lead to collinearity issues and increase model variance. 

If there are any near-zero variance predictors, they would be captured by the nearZeroVar function from the caret package.
```{r KJ 3.1.c.-analysis-nearzerovar}
nearZeroVar(Glass)
```

For highly correlated indicators, the cor() function from the caret package can be used. 
```{r KJ 3.1.c.-analysis-correlation}
Glass <- Glass %>% 
  select(-Type)
corr <- stats::cor(Glass)
dim(corr)
```

# KJ 3.2 
The soybean data can also be found at the UC Irvine Machine Learning Repository. Data were collected to predict disease in 683 soybeans. The 35 predictors are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left spots, mold growth). The outcome labels consist of 19 distinct classes.

```{r KJ.3.2-intro}
data(Soybean)
```

## KJ 3.2.a. Investigate the frequency distributions for the categorical predictors. Are any of the distributions degenerate in the ways discussed earlier in this chapter?

The "leaf.mild", "mycelium", and "seclerotia" have near zero variance. 
```{r KJ.3.2.a-analysis}
nearZeroVar(Soybean)
colnames(Soybean[,c(19,26,28)])

ggplot(Soybean, aes(Class)) +
  geom_histogram(stat="count", fill = "steelblue", color = "black") + 
  coord_flip()
```

## KJ 3.2.b. Roughly 18% of the data are missing. Are there particular predictors that are more likely to be missing? Is the pattern of missing data related to the classes?

### KJ 3.2.b. Approach
- Check for missing values
- Plot the missing data

```{r KJ.3.2.b-analysis}
# Check for missing values
sum_missing_sb <- map_dfr(names(Soybean), function(var) {
  col_data <- Soybean[[var]]
  tibble(
    "indicator" = var,
    "Total" = sum(!is.na(col_data)),
    "NA" = sum(is.na(col_data)),
    "PropNA" = sum(is.na(col_data))/sum(!is.na(col_data)),
    "NotMissing" = abs(sum(!is.na(col_data))-sum(is.na(col_data))),
    "PropNotMissing" = abs(sum(!is.na(col_data))-sum(is.na(col_data)))/sum(!is.na(col_data))
  )
})

missing_plot_data <- sum_missing_sb %>%
  select(-PropNotMissing,-PropNA,-Total) %>% 
  pivot_longer(cols = c( "NA",
                        "NotMissing"), 
               names_to = "type", 
               values_to = "count") 

ggplot(missing_plot_data, aes(x = reorder(indicator, -count, sum), y = count, fill = type)) + 
  geom_bar(position="fill", stat="identity")+
  coord_flip()+ 
  labs(
    title = "Proportion of Missing Indicator Values",
    x ="Indicator",
    y = "Proportion"
  )
```

## KJ 3.2.c. Develop a strategy for handling missing data, either by eliminating predictors or imputation.

### KJ KJ 3.2.c. Approach
To develop a strategy for handling missing data, this analysis will explore both approaches for handling missing data (i.e., eliminating predictors or imputation).

### KJ 3.2.c. Analysis

```{r KJ.3.2.c-analysis-impute}
sb_cleaned <- Soybean %>% 
  select(-Class) %>% 
  mutate_if(is.factor, as.numeric)

sb_processed <- preProcess(sb_cleaned, method='knnImpute')
```

If we wanted to eliminate the indicators that have a significant amount of missing data, we could simply remove the variables from the dataset:
```{r KJ.3.2.c-analysis-rem-missing}
problem_indicators <- sum_missing_sb %>% 
  group_by(indicator) %>% 
  filter(PropNA > .2)
print(problem_indicators$indicator)

# 'hail', 'sever', 'seed.tmt', 'lodging' are removed because the proportion of missing
# values is over 20%
sb_cleaned_opt2 <- Soybean %>% 
  select(-hail, -sever, -seed.tmt, -lodging)
```

# HA 6.2
The plastics data set consists of the monthly sales (in thousands) of product A for a plastics manufacturer for five years.

## HA 6.2.a Plot the time series of sales of product A. Can you identify seasonal fluctuations and/or a trend-cycle?

### HA 6.2.a Approach
Use autoplot() to visualize the time series and look for repeating seasonal patterns and an overall trend.

### HA 6.2.a. Analysis

```{r HA.6.2.a.-load-package-data}
library(fpp2)  # Loads 'plastics' dataset and required tools
```

```{r HA.6.2.a.-autoplot}
autoplot(plastics) +
  ggtitle("Monthly Sales of Product A") +
  xlab("Year") + ylab("Sales (in thousands)")
```

## HA 6.2.b Use a classical multiplicative decomposition to calculate the trend-cycle and seasonal indices.

### HA 6.2.b Approach
Ppply decompose() with the type = "multiplicative" option to break the series into trend, seasonal, and remainder components.

### HA 6.2.b. Analysis
```{r HA.6.2.b.-decomp}
decomp_plastics <- decompose(plastics, type = "multiplicative")
autoplot(decomp_plastics)
```


## HA 6.2.c Do the results support the graphical interpretation from part a?

### HA 6.2.c Approach
Visually compare the decomposition components with my initial impressions from the time series plot.

### HA 6.2.c. Analysis
The decomposition confirms the graphical interpretation: there is a strong seasonal pattern that repeats annually, and a trend-cycle showing an upward movement in sales over the five-year period.

## HA 6.2.d Compute and plot the seasonally adjusted data.

### HA 6.2.d Approach
Divide the original data by the seasonal component to obtain seasonally adjusted values.

### HA 6.2.d. Analysis
```{r}
adjusted_plastics <- seasadj(decomp_plastics)
autoplot(adjusted_plastics) +
  ggtitle("Seasonally Adjusted Sales of Product A")
```
The outlier significantly distorts the trend-cycle and irregular components, introducing noise into the decomposition.

## HA 6.2.e Change one observation to be an outlier (e.g., add 500 to one observation), and recompute the seasonally adjusted data. What is the effect of the outlier?

### HA 6.2.e. Approach
Introduce an outlier in the middle of the series and observe how it affects the seasonally adjusted values.

### HA 6.2.e. Analysis
```{r HA.6.2.e.-plastics-outlier}
plastics_outlier <- plastics
plastics_outlier[30] <- plastics_outlier[30] + 500  # Introduce outlier

decomp_outlier <- decompose(plastics_outlier, type = "multiplicative")
adjusted_outlier <- seasadj(decomp_outlier)

autoplot(adjusted_outlier) +
  ggtitle("Seasonally Adjusted Series with Outlier")
```

## HA 6.2.f Does it make any difference if the outlier is near the end rather than in the middle of the time series?

### HA 6.2.f. Approach
Compare the impact of placing an outlier near the end of the series instead of the middle.

### HA 6.2.f. Analysis
```{r HA.6.2.f.-analysis}
plastics_outlier_end <- plastics
plastics_outlier_end[55] <- plastics_outlier_end[55] + 500  # Outlier near end

decomp_outlier_end <- decompose(plastics_outlier_end, type = "multiplicative")
adjusted_outlier_end <- seasadj(decomp_outlier_end)

autoplot(adjusted_outlier_end) +
  ggtitle("Seasonally Adjusted Series with Outlier Near End")
```
Outliers near the end may have a more pronounced impact on the final few trend estimates and make the decomposition less reliable in that region due to fewer surrounding data points for smoothing.

# HA 7.5
Data set books contains the daily sales of paperback and hardcover books at the same store. The task is to forecast the next four days’ sales for paperback and hardcover books.

## HA 7.5.a. Plot the series and discuss the main features of the data.

```{r HA.7.5-load-data}
data(books)
```
### HA 7.5.a. Approach
Step 1: Determine the main features of the data by examining the:
(1) Trend
(2) Seasonality
(3) Heteroscedasticity
(4) Level shifts or structural changes

```{r HA.7.5.a-analysis}
autoplot(books)+
  ggtitle("Sales of Paperback and Hardcover Books")+
  xlab("Days")+
  ylab("Count")
# There is an upward trend. 

length(books)
frequency(books)
summary(books)

books_plot <- books %>%
  as.data.frame() %>%
  pivot_longer(cols = everything(), names_to = "Type", values_to = "Sales")

# Distribution of Book Sales 
# This is showing that hardcover is slightly right skewed.
ggplot(books_plot, aes(x = Sales, fill = Type)) +
  geom_histogram(aes(y = ..density..),alpha = 0.6, position = "identity", bins = 15) +
  geom_density(alpha = 0.2, color = "red") +
  facet_wrap(~Type) +
  labs(title = "Distribution of Book Sales by Type",
       x = "Number of Sales per Day", y = "Frequency") +
  theme_minimal()
```


## HA 7.5.b. Use the ses() function to forecast each series, and plot the forecasts.

## HA 7.5.b. Approach
Use simple exponential smoothing via ses() to forecast the next 4 days for both paperback and hardcover sales.

## HA 7.5.b. Analysis
```{r HA.7.5.b-analysis}
# Forecast 4 days ahead using Simple Exponential Smoothing
fc_paperback <- ses(books[, "Paperback"], h = 4)
fc_hardcover <- ses(books[, "Hardcover"], h = 4)

# Plot forecasts
autoplot(fc_paperback) + ggtitle("SES Forecast - Paperback Books")
autoplot(fc_hardcover) + ggtitle("SES Forecast - Hardcover Books")
```
These plots show the forecasted values with prediction intervals assuming the level is the only component modeled.

## HA 7.5.c. Compute the RMSE values for the training data in each case.

## HA 7.5.c. Approach
Extract the RMSE from the accuracy measures of the SES models.

## HA 7.5.c. Analysis
```{r HA.7.5.c-analysis}
# Compute RMSE for the training data
accuracy(fc_paperback)[2]  # RMSE for Paperback
accuracy(fc_hardcover)[2]  # RMSE for Hardcover
```
Since the RMSE is slightly lower for hardcover books, the SES model performs marginally better for that series.

# HA 7.6
We will continue with the daily sales of paperback and hardcover books in data set books.

## HA 7.6.a. Apply Holt’s linear method to the paperback and hardback series and compute four-day forecasts in each case.

### HA 7.6.a. Approach
Use the holt() function to compute four-day forecasts for both series.

### HA 7.6.a. Analysis
```{r HA.7.6.a.-analysis}
# Holt’s linear method
fc_holt_paperback <- holt(books[, "Paperback"], h = 4)
fc_holt_hardcover <- holt(books[, "Hardcover"], h = 4)

# Plot the forecasts
autoplot(fc_holt_paperback) + ggtitle("Holt Forecast - Paperback Books")
autoplot(fc_holt_hardcover) + ggtitle("Holt Forecast - Hardcover Books")
```

## HA 7.6.b. Compare the RMSE measures of Holt’s method for the two series to those of simple exponential smoothing in the previous question. Discuss the merits of the two forecasting methods for these data sets.
(Remember that Holt’s method is using one more parameter than SES.) 

### HA 7.6.b. Approach
Extract the RMSE values from Holt’s forecasts and compare them to the SES results to evaluate which model fits better.

### HA 7.6.b. Analysis
```{r HA.7.6.b-analysis}
# Get RMSE values for Holt's method
rmse_holt_paperback <- accuracy(fc_holt_paperback)[2]  # RMSE for Paperback
rmse_holt_hardcover <- accuracy(fc_holt_hardcover)[2]  # RMSE for Hardcover

rmse_holt_paperback
rmse_holt_hardcover
```
Holt’s method slightly reduced the RMSE for both series compared to SES, suggesting that modeling the trend improves forecast accuracy.

## HA 7.6.c. Compare the forecasts for the two series using both methods. Which do you think is best?

### HA 7.6.c. Approach
Visually and numerically compare the forecast values from SES and Holt’s method for each series.
### HA 7.6.c. Analysis
```{r HA.7.6.c-analysis}
# SES forecasts from earlier (assumed to be saved as fc_paperback and fc_hardcover)
autoplot(books[, "Paperback"]) +
  autolayer(fc_paperback, series = "SES", PI = FALSE) +
  autolayer(fc_holt_paperback$mean, series = "Holt", PI = FALSE) +
  ggtitle("Comparison of SES and Holt Forecasts - Paperback")

autoplot(books[, "Hardcover"]) +
  autolayer(fc_hardcover, series = "SES", PI = FALSE) +
  autolayer(fc_holt_hardcover$mean, series = "Holt", PI = FALSE) +
  ggtitle("Comparison of SES and Holt Forecasts - Hardcover")
```
The Holt forecasts capture the direction of the trend more effectively than SES. Given the small reduction in RMSE and better trend alignment, Holt’s method appears better suited for both book series.

## HA 7.6.d. Calculate a 95% prediction interval for the first forecast for each series, using the RMSE values and assuming normal errors. Compare your intervals with those produced using ses and holt.

### HA 7.6.d. Approach
Use the RMSE values to calculate manual 95% prediction intervals for the first forecast assuming normal distribution of errors.

### HA 7.6.d. Analysis
```{r HA.7.6.d-analysis}
# Get point forecasts
point_fc_pb_ses <- fc_paperback$mean[1]
point_fc_pb_holt <- fc_holt_paperback$mean[1]

point_fc_hc_ses <- fc_hardcover$mean[1]
point_fc_hc_holt <- fc_holt_hardcover$mean[1]

# Get RMSE
rmse_pb_ses <- 33.64
rmse_pb_holt <- as.numeric(rmse_holt_paperback)

rmse_hc_ses <- 31.93
rmse_hc_holt <- as.numeric(rmse_holt_hardcover)

# Compute 95% prediction intervals manually: point forecast ± 1.96 * RMSE
interval_pb_ses <- c(point_fc_pb_ses - 1.96 * rmse_pb_ses,
                     point_fc_pb_ses + 1.96 * rmse_pb_ses)

interval_pb_holt <- c(point_fc_pb_holt - 1.96 * rmse_pb_holt,
                      point_fc_pb_holt + 1.96 * rmse_pb_holt)

interval_hc_ses <- c(point_fc_hc_ses - 1.96 * rmse_hc_ses,
                     point_fc_hc_ses + 1.96 * rmse_hc_ses)

interval_hc_holt <- c(point_fc_hc_holt - 1.96 * rmse_hc_holt,
                      point_fc_hc_holt + 1.96 * rmse_hc_holt)

interval_pb_ses
interval_pb_holt
interval_hc_ses
interval_hc_holt

# Check model-based intervals
fc_paperback$lower[1,]
fc_paperback$upper[1,]

fc_holt_paperback$lower[1,]
fc_holt_paperback$upper[1,]
```
The calculated 95% prediction intervals closely match those generated by ses() and holt(), which supports the assumption of normally distributed forecast errors. 

# HA 8.1
Figure 8.31 shows the ACFs for 36 random numbers, 360 random numbers and 1,000 random numbers.

## HA 8.1.a. Explain the differences among these figures. Do they all indicate that the data are white noise?

### HA 8.1.a. Approach
Simulate three white noise series with different sample sizes and compare their ACF plots to assess randomness and variability.

### HA 8.1.a. Analysis
```{r HA.8.1.a-analysis}
library(fpp2)

set.seed(123)
wn_36 <- ts(rnorm(36))
wn_360 <- ts(rnorm(360))
wn_1000 <- ts(rnorm(1000))

par(mfrow = c(1, 3))  # Side-by-side plots
Acf(wn_36, main = "White Noise (n = 36)")
Acf(wn_360, main = "White Noise (n = 360)")
Acf(wn_1000, main = "White Noise (n = 1000)")
```
All three ACF plots represent white noise, as there are no significant autocorrelations beyond the 95% confidence bounds. However, the plots differ visually: the small-sample (n=36) ACF shows more random spikes, while the larger samples (n=360, n=1000) produce ACFs that cluster more closely around zero. This is because smaller samples have greater sampling variability.


## HA 8.1.b. Why are the critical values at different distances from the mean of zero? Why are the autocorrelations different in each figure when they each refer to white noise?
The blue dashed lines on the ACF plots show the range where we expect the values to fall if the data are truly random. When we have more data points, this range gets smaller, making it easier to spot unusual patterns. With fewer data points, the range is wider, so the values can bounce around more just by chance.

All three series are random numbers but the bars on the plots don’t look exactly the same. This is because when you work with small sets of data, the results can look more random or uneven. With larger sets of data, things tend to even out, and the bars stay closer to zero.

So, while the three plots look a little different, they all show the same thing: random data with no real pattern. The differences happen just because of the number of data points used.