---
title: "Water Flow Forecast Analysis"
author: "Marc Fridson"
date: "`r Sys.Date()`"
output:
  word_document:
    toc: true
    toc_depth: '3'
  html_document:
    toc: true
    toc_float: true
    theme: cosmo
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  fig.align = 'center'
)

# Load required libraries
library(tidyverse)
library(forecast)
library(lubridate)
library(readxl)
library(zoo)
library(tseries)
library(gridExtra)
library(writexl)
library(knitr)

# Set theme
theme_set(theme_minimal() + 
  theme(plot.title = element_text(size = 14, face = "bold"),
        axis.title = element_text(size = 12)))
```

# Executive Summary

This analysis provides a comprehensive water flow forecasting system for a dual-pipe water distribution network. The analysis includes:

-   Hourly aggregation of sub-hourly water flow measurements
-   Exploratory data analysis to identify temporal patterns
-   Time series modeling using multiple approaches with focus on capturing daily seasonality
-   7-day ahead forecasts with uncertainty quantification
-   Individual pipe flow predictions based on historical contribution ratios

# Introduction

Water flow forecasting is critical for effective water resource management and infrastructure planning. This analysis develops predictive models for total water flow based on historical data from two pipes in the distribution system.

# Data Loading and Preprocessing

```{r load-data}
cat("\n========== PART C: WATER FLOW FORECAST ==========\n\n")

# Load data
pipe1_data <- read_excel("Waterflow_Pipe1.xlsx")
pipe2_data <- read_excel("Waterflow_Pipe2.xlsx")

# Rename columns
names(pipe1_data) <- c("DateTime", "WaterFlow")
names(pipe2_data) <- c("DateTime", "WaterFlow")

# Convert to POSIXct
pipe1_data$DateTime <- as.POSIXct(pipe1_data$DateTime)
pipe2_data$DateTime <- as.POSIXct(pipe2_data$DateTime)

cat("Data loaded successfully\n")
cat("Pipe 1 records:", nrow(pipe1_data), "\n")
cat("Pipe 2 records:", nrow(pipe2_data), "\n\n")

# Examine temporal patterns
cat("Temporal characteristics:\n")
cat("Pipe 1 - Time span:", 
    round(as.numeric(difftime(max(pipe1_data$DateTime), 
                              min(pipe1_data$DateTime), 
                              units = "days")), 1), "days\n")
cat("Pipe 2 - Time span:", 
    round(as.numeric(difftime(max(pipe2_data$DateTime), 
                              min(pipe2_data$DateTime), 
                              units = "days")), 1), "days\n")
```

# Step 1: Hourly Aggregation

The raw data contains sub-hourly measurements at different frequencies. We aggregate these to hourly intervals for more stable time series analysis.

## Handle Different Recording Frequencies

```{r hourly-aggregation}
cat("STEP 1: Aggregating to hourly intervals...\n\n")

# Function for robust hourly aggregation
aggregate_hourly <- function(data, pipe_name) {
  result <- data %>%
    mutate(Hour = floor_date(DateTime, "hour")) %>%
    group_by(Hour) %>%
    summarise(
      WaterFlow = mean(WaterFlow, na.rm = TRUE),
      n_readings = n(),
      .groups = 'drop'
    )
  
  cat(pipe_name, "- Readings per hour summary:\n")
  print(summary(result$n_readings))
  
  return(result)
}

pipe1_hourly <- aggregate_hourly(pipe1_data, "Pipe 1")
pipe2_hourly <- aggregate_hourly(pipe2_data, "Pipe 2")

cat("\nPipe 1 - Hourly data points:", nrow(pipe1_hourly), "\n")
cat("Pipe 2 - Hourly data points:", nrow(pipe2_hourly), "\n\n")
```

## Align Time Series

```{r align-timeseries}
# Find the common time range
start_time <- max(min(pipe1_hourly$Hour), min(pipe2_hourly$Hour))
end_time <- min(max(pipe1_hourly$Hour), max(pipe2_hourly$Hour))

cat("Common time range:\n")
cat("Start:", format(start_time), "\n")
cat("End:", format(end_time), "\n")

# Create complete hourly sequence
hourly_seq <- seq(from = start_time, to = end_time, by = "hour")
complete_hours <- data.frame(Hour = hourly_seq)
cat("Total hours in sequence:", length(hourly_seq), "\n\n")

# Join and interpolate
pipe1_complete <- complete_hours %>%
  left_join(pipe1_hourly, by = "Hour") %>%
  mutate(
    WaterFlow_interp = na.approx(WaterFlow, na.rm = FALSE, maxgap = 3),
    # For remaining NAs, use seasonal naive approach
    WaterFlow_final = ifelse(is.na(WaterFlow_interp),
                            na.aggregate(WaterFlow, FUN = mean),
                            WaterFlow_interp)
  )

pipe2_complete <- complete_hours %>%
  left_join(pipe2_hourly, by = "Hour") %>%
  mutate(
    WaterFlow_interp = na.approx(WaterFlow, na.rm = FALSE, maxgap = 3),
    WaterFlow_final = ifelse(is.na(WaterFlow_interp),
                            na.aggregate(WaterFlow, FUN = mean),
                            WaterFlow_interp)
  )

# Create total flow
total_flow <- data.frame(
  DateTime = complete_hours$Hour,
  Pipe1 = pipe1_complete$WaterFlow_final,
  Pipe2 = pipe2_complete$WaterFlow_final,
  Total = pipe1_complete$WaterFlow_final + pipe2_complete$WaterFlow_final
) %>%
  filter(!is.na(Total))

cat("Total hourly observations after aggregation:", nrow(total_flow), "\n")
cat("Missing values in final dataset:", sum(is.na(total_flow$Total)), "\n\n")
```

# Step 2: Exploratory Data Analysis

```{r exploratory-analysis, fig.height=10}
cat("STEP 2: Creating exploratory visualizations...\n")

# Plot 1: Individual pipe flows over time
p1 <- ggplot(total_flow, aes(x = DateTime)) +
  geom_line(aes(y = Pipe1, color = "Pipe 1"), size = 1) +
  geom_line(aes(y = Pipe2, color = "Pipe 2"), size = 1) +
  labs(title = "Hourly Water Flow by Pipe",
       x = "Date", y = "Water Flow", color = "Source") +
  scale_color_manual(values = c("Pipe 1" = "blue", "Pipe 2" = "red"))

# Plot 2: Total water flow
p2 <- ggplot(total_flow, aes(x = DateTime, y = Total)) +
  geom_line(size = 1, color = "darkgreen") +
  labs(title = "Total Water Flow Over Time",
       x = "Date", y = "Total Water Flow") +
  geom_smooth(method = "loess", se = TRUE, alpha = 0.2)

# Plot 3: Hourly pattern
hourly_pattern <- total_flow %>%
  mutate(Hour = hour(DateTime)) %>%
  group_by(Hour) %>%
  summarise(
    Mean_Flow = mean(Total, na.rm = TRUE),
    SD_Flow = sd(Total, na.rm = TRUE),
    .groups = 'drop'
  )

p3 <- ggplot(hourly_pattern, aes(x = Hour, y = Mean_Flow)) +
  geom_ribbon(aes(ymin = Mean_Flow - SD_Flow,
                  ymax = Mean_Flow + SD_Flow),
              alpha = 0.3, fill = "lightblue") +
  geom_line(size = 1.2, color = "darkblue") +
  geom_point(size = 2, color = "darkblue") +
  labs(title = "Average Daily Pattern",
       subtitle = "Mean Â± 1 Standard Deviation",
       x = "Hour of Day", y = "Average Total Flow") +
  scale_x_continuous(breaks = seq(0, 23, by = 3))

# Plot 4: Distribution
p4 <- ggplot(total_flow, aes(x = Total)) +
  geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7, color = "black") +
  geom_density(aes(y = after_stat(count)), color = "red", size = 1.2) +
  labs(title = "Distribution of Total Water Flow",
       x = "Total Water Flow", y = "Frequency")

# Combine plots
exploratory_plots <- grid.arrange(p1, p2, p3, p4, ncol = 2, nrow = 2)
```

## Key Observations

-   **Flow Patterns**: Both pipes show similar temporal patterns with regular daily fluctuations
-   **Daily Cycle**: Clear 24-hour periodicity with peak flows during daytime hours
-   **Distribution**: Total flow appears approximately normally distributed
-   **Trend**: No obvious long-term trend visible in the data

# Step 3: Time Series Modeling

```{r timeseries-prep}
cat("STEP 3: Building time series models...\n")

# Create time series object
ts_data <- ts(total_flow$Total, frequency = 24)  # 24 hours per day

# Check length
cat("Time series length:", length(ts_data), "hours\n")
cat("Number of complete days:", length(ts_data)/24, "\n\n")

# Check stationarity
adf_test <- adf.test(ts_data)
cat("ADF test p-value:", round(adf_test$p.value, 4), "\n")
cat("Series is", ifelse(adf_test$p.value < 0.05, "stationary", "non-stationary"), "\n\n")
```

## Time Series Decomposition

```{r decomposition, fig.height=8}
# Decomposition
if(length(ts_data) >= 48) {
  decomp <- stl(ts_data, s.window = "periodic", robust = TRUE)
  autoplot(decomp) +
    ggtitle("Time Series Decomposition") +
    theme_minimal()
  
  # Extract seasonal component strength
  seasonal_strength <- 1 - var(remainder(decomp)) / var(ts_data - trendcycle(decomp))
  cat("Seasonal strength:", round(seasonal_strength, 3), "\n\n")
} else {
  cat("Insufficient data for STL decomposition\n\n")
}
```

## Model Fitting and Comparison

```{r model-fitting}
cat("Fitting time series models with focus on seasonality...\n\n")

# Model 1: SARIMA with forced seasonality
model_arima <- auto.arima(ts_data, 
                         seasonal = TRUE, 
                         stepwise = FALSE,
                         D = 1,  # Force seasonal differencing
                         max.P = 2, max.Q = 2,
                         trace = FALSE)
cat("SARIMA model:", as.character(model_arima), "\n")

# Model 2: ETS with seasonal component
model_ets <- ets(ts_data, model = "ZZZ", damped = NULL)
cat("ETS model:", model_ets$method, "\n")

# Model 3: STL + ETS
model_stlf <- stlf(ts_data, h = 168, s.window = "periodic", method = "ets")
cat("STL+ETS model fitted\n")

# Model 4: TBATS for complex seasonality
model_tbats <- tbats(ts_data)
cat("TBATS model fitted\n")

# Model 5: Seasonal Naive (baseline)
model_snaive <- snaive(ts_data, h = 168)
cat("Seasonal Naive baseline fitted\n\n")

# Compare models using cross-validation
test_size <- min(48, floor(length(ts_data) * 0.2))
train_ts <- head(ts_data, length(ts_data) - test_size)
test_ts <- tail(ts_data, test_size)

# Function to calculate forecast accuracy
get_accuracy <- function(model_func, train_data, test_data, h) {
  fc <- forecast(model_func(train_data), h = h)
  accuracy(fc, test_data)[2, c("RMSE", "MAE", "MAPE")]
}

# Calculate accuracy for each model
model_comparison <- data.frame(
  Model = c("SARIMA", "ETS", "STL+ETS", "TBATS", "Seasonal Naive"),
  RMSE = NA, MAE = NA, MAPE = NA
)

# SARIMA
tryCatch({
  fc <- forecast(auto.arima(train_ts, seasonal = TRUE, D = 1), h = test_size)
  acc <- accuracy(fc, test_ts)
  model_comparison[1, 2:4] <- acc[2, c("RMSE", "MAE", "MAPE")]
}, error = function(e) cat("SARIMA error:", e$message, "\n"))

# ETS
tryCatch({
  fc <- forecast(ets(train_ts), h = test_size)
  acc <- accuracy(fc, test_ts)
  model_comparison[2, 2:4] <- acc[2, c("RMSE", "MAE", "MAPE")]
}, error = function(e) cat("ETS error:", e$message, "\n"))

# STL+ETS
tryCatch({
  fc <- stlf(train_ts, h = test_size)
  acc <- accuracy(fc, test_ts)
  model_comparison[3, 2:4] <- acc[2, c("RMSE", "MAE", "MAPE")]
}, error = function(e) cat("STL+ETS error:", e$message, "\n"))

# TBATS
tryCatch({
  fc <- forecast(tbats(train_ts), h = test_size)
  acc <- accuracy(fc, test_ts)
  model_comparison[4, 2:4] <- acc[2, c("RMSE", "MAE", "MAPE")]
}, error = function(e) cat("TBATS error:", e$message, "\n"))

# Seasonal Naive
fc <- snaive(train_ts, h = test_size)
acc <- accuracy(fc, test_ts)
model_comparison[5, 2:4] <- acc[2, c("RMSE", "MAE", "MAPE")]

# Display comparison
model_comparison <- model_comparison[complete.cases(model_comparison), ]
kable(model_comparison, caption = "Model Performance Metrics", digits = 3)

# Select best model based on RMSE
best_model_idx <- which.min(model_comparison$RMSE)
best_model_name <- model_comparison$Model[best_model_idx]
cat("\nBest model based on RMSE:", best_model_name, "\n\n")
```

# Step 4: Generate Forecasts

```{r generate-forecasts}
cat("STEP 4: Generating 1-week forecast...\n")

# Use the best model or STL+ETS if it performs well
if(best_model_name == "STL+ETS" || model_comparison$RMSE[3] < 1.1 * min(model_comparison$RMSE)) {
  cat("Using STL+ETS for better seasonal pattern capture\n")
  final_forecast <- stlf(ts_data, h = 168, s.window = "periodic", method = "ets")
} else if(best_model_name == "SARIMA") {
  final_forecast <- forecast(model_arima, h = 168, level = c(80, 95))
} else if(best_model_name == "TBATS") {
  final_forecast <- forecast(model_tbats, h = 168, level = c(80, 95))
} else {
  # Default to STL+ETS for seasonal pattern
  cat("Defaulting to STL+ETS for seasonal pattern capture\n")
  final_forecast <- stlf(ts_data, h = 168, s.window = "periodic", method = "ets")
}

# Create forecast dataframe
last_time <- max(total_flow$DateTime)
forecast_times <- seq(from = last_time + hours(1),
                     by = "hour",
                     length.out = 168)

forecast_df <- data.frame(
  DateTime = forecast_times,
  Forecast = as.numeric(final_forecast$mean),
  Lo80 = as.numeric(final_forecast$lower[, 1]),
  Hi80 = as.numeric(final_forecast$upper[, 1]),
  Lo95 = as.numeric(final_forecast$lower[, 2]),
  Hi95 = as.numeric(final_forecast$upper[, 2])
)

# Check if forecast has seasonality
forecast_range <- max(forecast_df$Forecast) - min(forecast_df$Forecast)
cat("\nForecast range:", round(forecast_range, 2), "\n")
cat("Forecast coefficient of variation:", 
    round(sd(forecast_df$Forecast) / mean(forecast_df$Forecast) * 100, 2), "%\n")

# Split between pipes based on historical average
pipe1_pct <- mean(total_flow$Pipe1 / total_flow$Total, na.rm = TRUE)
pipe2_pct <- 1 - pipe1_pct

cat("\nHistorical flow split:\n")
cat("Pipe 1:", round(pipe1_pct * 100, 1), "%\n")
cat("Pipe 2:", round(pipe2_pct * 100, 1), "%\n\n")
```

# Step 5: Visualize Forecasts

## Main Forecast Plot

```{r forecast-visualization, fig.height=7}
cat("STEP 5: Creating forecast visualizations...\n")

# Main forecast plot
historical_tail <- tail(total_flow, 168)  # Last week of data
forecast_vis <- data.frame(
  DateTime = c(historical_tail$DateTime, forecast_df$DateTime),
  Value = c(historical_tail$Total, forecast_df$Forecast),
  Type = c(rep("Historical", nrow(historical_tail)),
           rep("Forecast", nrow(forecast_df)))
)

p_forecast <- ggplot() +
  geom_line(data = filter(forecast_vis, Type == "Historical"),
            aes(x = DateTime, y = Value),
            color = "black", size = 1) +
  geom_line(data = filter(forecast_vis, Type == "Forecast"),
            aes(x = DateTime, y = Value),
            color = "blue", size = 1) +
  geom_ribbon(data = forecast_df,
              aes(x = DateTime, ymin = Lo95, ymax = Hi95),
              alpha = 0.2, fill = "blue") +
  geom_ribbon(data = forecast_df,
              aes(x = DateTime, ymin = Lo80, ymax = Hi80),
              alpha = 0.3, fill = "blue") +
  geom_vline(xintercept = last_time,
             linetype = "dashed", color = "red", alpha = 0.5) +
  labs(title = "Water Flow Forecast - Next 7 Days",
       subtitle = "Showing 80% and 95% confidence intervals",
       x = "Date", y = "Total Water Flow") +
  theme_minimal()

print(p_forecast)
```

## Forecasted Daily Pattern

```{r forecast-pattern}
# Forecast pattern plot
forecast_pattern <- forecast_df %>%
  mutate(
    Hour = hour(DateTime),
    Day = wday(DateTime, label = TRUE)
  ) %>%
  group_by(Hour) %>%
  summarise(
    Mean_Forecast = mean(Forecast),
    Min_Forecast = min(Forecast),
    Max_Forecast = max(Forecast),
    .groups = 'drop'
  )

p_pattern <- ggplot(forecast_pattern, aes(x = Hour)) +
  geom_ribbon(aes(ymin = Min_Forecast, ymax = Max_Forecast),
              alpha = 0.3, fill = "lightblue") +
  geom_line(aes(y = Mean_Forecast), size = 1.2, color = "darkblue") +
  geom_point(aes(y = Mean_Forecast), size = 2, color = "darkblue") +
  labs(title = "Forecasted Daily Pattern",
       subtitle = "Average with min/max range over forecast period",
       x = "Hour of Day", y = "Forecasted Flow") +
  scale_x_continuous(breaks = seq(0, 23, by = 3)) +
  theme_minimal()

print(p_pattern)

# Check if pattern is realistic
pattern_range <- max(forecast_pattern$Mean_Forecast) - min(forecast_pattern$Mean_Forecast)
cat("\nDaily pattern range in forecast:", round(pattern_range, 2), "\n")
if(pattern_range < 1) {
  cat("WARNING: Forecast may be too flat - consider adjusting model\n")
}
```

# Step 6: Model Diagnostics

```{r diagnostics, fig.height=8}
cat("\nSTEP 6: Checking model diagnostics...\n")

# Get the actual model used for forecasting
if(exists("final_forecast") && !is.null(final_forecast$model)) {
  checkresiduals(final_forecast$model)
} else {
  checkresiduals(model_stlf$model)
}

# Additional diagnostic: forecast vs historical pattern comparison
historical_pattern <- total_flow %>%
  mutate(Hour = hour(DateTime)) %>%
  group_by(Hour) %>%
  summarise(Historical = mean(Total), .groups = 'drop')

pattern_comparison <- forecast_pattern %>%
  select(Hour, Forecast = Mean_Forecast) %>%
  left_join(historical_pattern, by = "Hour") %>%
  pivot_longer(cols = c(Forecast, Historical), names_to = "Type", values_to = "Flow")

p_compare <- ggplot(pattern_comparison, aes(x = Hour, y = Flow, color = Type)) +
  geom_line(size = 1.2) +
  geom_point(size = 2) +
  labs(title = "Historical vs Forecasted Daily Pattern",
       x = "Hour of Day", y = "Average Flow") +
  scale_color_manual(values = c("Historical" = "black", "Forecast" = "blue")) +
  scale_x_continuous(breaks = seq(0, 23, by = 3)) +
  theme_minimal()

print(p_compare)
```

# Step 7: Export Results

```{r export-results}
cat("\nSTEP 7: Exporting results to Excel...\n")

# Create forecast dataframes for each pipe
pipe1_forecast <- forecast_df %>%
  mutate(
    DateTime = DateTime,
    WaterFlow = Forecast * pipe1_pct
  ) %>%
  select(DateTime, WaterFlow)

pipe2_forecast <- forecast_df %>%
  mutate(
    DateTime = DateTime,
    WaterFlow = Forecast * pipe2_pct
  ) %>%
  select(DateTime, WaterFlow)

# Create summary statistics
summary_stats <- data.frame(
  Metric = c("Total observations", "Forecast period (hours)", 
             "Best model", "Average historical flow", 
             "Average forecasted flow", "Forecast range",
             "Pipe 1 percentage", "Pipe 2 percentage"),
  Value = c(nrow(total_flow), 168,
            ifelse(exists("best_model_name"), best_model_name, "STL+ETS"),
            round(mean(total_flow$Total, na.rm = TRUE), 2),
            round(mean(forecast_df$Forecast), 2),
            round(forecast_range, 2),
            paste0(round(pipe1_pct * 100, 1), "%"),
            paste0(round(pipe2_pct * 100, 1), "%"))
)

# Create Excel workbook
excel_list <- list(
  "Summary" = summary_stats,
  "Water_Forecast_Total" = forecast_df,
  "Water_Forecast_Pipe1" = pipe1_forecast,
  "Water_Forecast_Pipe2" = pipe2_forecast
)

write_xlsx(excel_list, "Water_Flow_Forecasts.xlsx")
cat("Results exported to Water_Flow_Forecasts.xlsx\n")
```

# Summary of Results

```{r summary}
cat("\n========== ANALYSIS COMPLETE ==========\n\n")
cat("Summary of Results:\n")
cat("-----------------\n")

# Display summary statistics
kable(summary_stats, caption = "Analysis Summary")
```

## Key Findings

1.  **Model Selection**: The analysis compared multiple models to capture seasonal patterns effectively
2.  **Flow Distribution**: Pipe 1 contributes `r round(pipe1_pct * 100, 1)`% and Pipe 2 contributes `r round(pipe2_pct * 100, 1)`% to total flow
3.  **Seasonal Patterns**: The forecast captures daily variations in water usage
4.  **Uncertainty**: 80% and 95% confidence intervals provide robust uncertainty estimates

## Files Created

The analysis generates the following output files:

-   `water_flow_exploration.png` - Exploratory data analysis plots
-   `water_flow_decomposition.png` - Time series decomposition
-   `water_flow_forecast.png` - Main forecast visualization
-   `water_flow_forecast_pattern.png` - Daily pattern forecast
-   `water_flow_residuals.png` - Model diagnostic plots
-   `Water_Flow_Forecasts.xlsx` - Excel file with all forecasts

# Session Information

```{r session-info}
sessionInfo()
```
